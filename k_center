import pickle
import random
import matplotlib
from side_function import covert_coordinate_from_4326_to_MA,maintenance_depots,find_furthest_point,\
    find_minimum_time_gap,MIC_analyses,circle_rank,read_raster,cal_cost,raster_coverage,draw


font = {'size'   : 24}
matplotlib.rc('font', **font)

def read_point_and_process():
    point_set = pickle.load(open('point_set.obj', 'rb'))
    start_time=[p[1] for p in point_set]
    duration=[p[2] for p in point_set]
    severity=[p[3] for p in point_set]
    sub_type=[p[4] for p in point_set]
    point_set=[p[0] for p in point_set]

    # print('incidents:',len(point_set))
    new_point_set=[(p[0]+random.uniform(-1e-09,1e-09),p[1]+random.uniform(-1e-09,1e-09)) for p in point_set]
    new_point_set=[covert_coordinate_from_4326_to_MA(p) for p in new_point_set]

    greedy_find(new_point_set,start_time,duration,severity,sub_type)
    # greedy_generate(point_set)

def greedy_find(point_set,start_time_set,duration_set,severity,sub_type):
    # raster_area1=read_raster('ForLowell/2%floodfinal.tif')
    # raster_area2=read_raster('ForLowell/bob9151mph1.tif')
    # raster_area3=read_raster('ForLowell/fld9951mph1.tif')
    m_d_set=maintenance_depots('Maintenance_Depots/Maintenance_Depots.shp')
    center_set_points=[]
    # included_set=[]
    # double_adjacent_set=[]
    ran_initial=random.randint(0,len(m_d_set))
    p_f = m_d_set[ran_initial]
    while p_f is not None:
        center_set_points.append(p_f)
        # included_set=find_included(p_f,point_set,included_set,r)
        # double_adjacent_set=find_included(p_f,point_set,double_adjacent_set,2*r)
        p_f = find_furthest_point(p_f,m_d_set,center_set_points,r)
    # center_set_points=[point_set[c_index] for c_index in center_set]
    center_set_points,original_point_set,key_center_set,min_gap,key_min,correlated_num,parallel_num,same_time_num=\
        exclude_unnecessary(center_set_points,point_set,start_time_set,duration_set,severity,sub_type,threshold=TH)

    # draw_figures(min_gap,key_min,correlated_num,parallel_num,same_time_num,r)
    # print('final stations: ',len(center_set_points))

    network_cost=cal_cost(same_time_num,r)
    print('linear cost:',network_cost[0],'\nexponential cost:',network_cost[1])

    # raster_coverage(center_set_points,key_center_set,raster_area1,'NOAA 2%',r)
    # raster_coverage(center_set_points,key_center_set,raster_area2,'Bob 1991',r)
    # raster_coverage(center_set_points,key_center_set,raster_area3,'Floyd 1999',r)
    draw(original_point_set,[p[0] for p in center_set_points],r,[p[1] for p in center_set_points])


def exclude_unnecessary(center_set,point_set,start_time_set,duration_set,severity,sub_type,threshold):
    key_threshold = r / 1000 / 1.60934 * 12
    flink = open('stations.txt','w+')
    original_point_set = point_set.copy()
    final_center_set = []
    # max_cov=threshold
    key_sum=0
    key_count=0
    key_center_set=[]
    min_t_gap_set=[]
    average_t_gap_set=[]
    key_min_t_gap_set=[]
    key_average_t_gap_set=[]
    correlated_incidents_set=[]
    parallel_incident_set=[]
    same_time_incident_set=[]
    MIC_full_info=[]
    while len(point_set)>(len(original_point_set)*threshold):
        circle_info = circle_rank(center_set, point_set,r)
        if not len(circle_info):
            break
        excluded_set=circle_info[1]
        final_center_set.append((circle_info[0],len(excluded_set)))
        min_t_gap,avg_t_gap,num_same_time_incident,correlated_incidents,parallel_incidents=find_minimum_time_gap\
            (start_time_set,duration_set,severity,sub_type,excluded_set,original_point_set)
        MIC_full_info+=correlated_incidents
        min_t_gap_set.append(min_t_gap)
        average_t_gap_set.append(avg_t_gap)
        correlated_incidents_set.append(len(correlated_incidents))
        parallel_incident_set.append(len(parallel_incidents))
        same_time_incident_set.append(num_same_time_incident)
        max_cov=len(excluded_set)
        if max_cov>=key_threshold:
            # print('position:',circle_info[0],'number of incidents:',max_cov,' min_time_gap:',min_t_gap,' mins')
            flink.write('%s,%s,%s,%s,k,%s\n' % (circle_info[0][0], circle_info[0][1], max_cov, min_t_gap,num_same_time_incident))
            key_center_set.append((circle_info[0], len(excluded_set)))
            key_sum+=max_cov
            key_count+=1
            key_min_t_gap_set.append(min_t_gap)
            key_average_t_gap_set.append(avg_t_gap)
        else:
            flink.write('%s,%s,%s,%s,r,%s\n' % (circle_info[0][0], circle_info[0][1], max_cov, min_t_gap,num_same_time_incident))
        excluded_set.sort(reverse=True)
        for point in excluded_set:
            point_set.remove(point)
    flink.close()
    # print('number of key stations:', key_count)
    # print('key stations coverage:', key_sum / len(original_point_set))
    # print('coverage:',1-len(point_set)/len(original_point_set))

    MIC_analyses(MIC_full_info)

    return final_center_set,original_point_set,key_center_set,\
           min_t_gap_set,key_min_t_gap_set,correlated_incidents_set,parallel_incident_set, same_time_incident_set


if __name__ == '__main__':
    global TH,r
    TH = 0.05
    for r in range(5,11):
        print('radius:',r)
        r = r * 1000
        r = r * 1.60934
        read_point_and_process()

